import streamlit as st
import numpy as np
import librosa
import joblib
import os
import torch
import re
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
from collections import OrderedDict
import requests
from sklearn.preprocessing import StandardScaler

# --- GPT2PPL Text Detection Class ---
class GPT2PPL:
    def __init__(self, model_id="gpt2"):
        self.model_id = model_id
        self.model = GPT2LMHeadModel.from_pretrained(model_id)
        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
        self.max_length = self.model.config.n_positions
        self.stride = 512

    def getResults(self, threshold):
        if threshold < 60:
            return "The Text is generated by AI.", 0
        elif threshold < 80:
            return "The Text most probably contains parts which are generated by AI. (requires more text for better judgment)", 0
        else:
            return "The Text is written by a Human.", 1

    def __call__(self, sentence):
        results = OrderedDict()
        total_valid_char = re.findall("[a-zA-Z0-9]+", sentence)
        total_valid_char = sum([len(x) for x in total_valid_char])
        if total_valid_char < 100:
            return {"status": "Please input more text (minimum 100 characters)"}, "Please input more text (minimum 100 characters)"

        lines = re.split(r'(?<=[.?!][ \[\(])|(?<=\n)\s*', sentence)
        lines = list(filter(lambda x: (x is not None) and (len(x) > 0), lines))
        ppl = self.getPPL(sentence)
        results["Perplexity"] = ppl

        Perplexity_per_line = []
        for line in lines:
            if re.search("[a-zA-Z0-9]+", line) is None:
                continue
            ppl = self.getPPL(line)
            Perplexity_per_line.append(ppl)

        results["Perplexity per line"] = sum(Perplexity_per_line) / len(Perplexity_per_line)
        results["Burstiness"] = max(Perplexity_per_line)
        out, label = self.getResults(results["Perplexity per line"])

        # Remove the "label" from the output
        return results, out

    def getPPL(self, sentence):
        encodings = self.tokenizer(sentence, return_tensors="pt")
        seq_len = encodings.input_ids.size(1)
        nlls = []
        prev_end_loc = 0

        for begin_loc in range(0, seq_len, self.stride):
            end_loc = min(begin_loc + self.max_length, seq_len)
            trg_len = end_loc - prev_end_loc
            input_ids = encodings.input_ids[:, begin_loc:end_loc]
            target_ids = input_ids.clone()
            target_ids[:, :-trg_len] = -100

            with torch.no_grad():
                outputs = self.model(input_ids, labels=target_ids)
                neg_log_likelihood = outputs.loss * trg_len

            nlls.append(neg_log_likelihood)
            prev_end_loc = end_loc
            if end_loc == seq_len:
                break

        ppl = int(torch.exp(torch.stack(nlls).sum() / end_loc))
        return ppl

# --- Humanize Text Function using RapidAPI ---
def humanize_text(text):
    url = "https://ai-content-detection-ai-detector-humanize-ai-text.p.rapidapi.com/humanizeContent"
    querystring = {"noqueue": "1", "language": "en"}

    payload = { "text": text }
    headers = {
        "x-rapidapi-key": "8cb242635cmsh0586bf53ae88654p17dca6jsnc70f965d8513",
        "x-rapidapi-host": "ai-content-detection-ai-detector-humanize-ai-text.p.rapidapi.com",
        "Content-Type": "application/json"
    }

    response = requests.post(url, json=payload, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()['result']['humanizedText']
    else:
        return f"Error humanizing text: {response.text}"

# --- Audio Detection Model ---
@st.cache_resource
def load_model_and_scaler():
    model_path = "svm_model.pkl"
    scaler_path = "scaler.pkl"
    
    if not os.path.exists(model_path) or not os.path.exists(scaler_path):
        st.error("Model files not found. Please train the model first.")
        return None, None
    
    model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)
    return model, scaler

# Extract MFCC features for audio detection
def extract_mfcc_features(audio_data, sr, n_mfcc=13, n_fft=2048, hop_length=512):
    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)
    return np.mean(mfccs.T, axis=0)

# --- Streamlit App Layout ---
def main():
    st.title("AI Content Detection - Text and Audio")

    # Choose between Text or Audio Detection
    option = st.selectbox("Choose the content to analyze", ("Text", "Audio"))

    if option == "Text":
        # Text Input
        st.header("AI Text Detection")
        text_input = st.text_area("Enter text for AI detection:")

        if text_input:
            detector = GPT2PPL()
            results, verdict = detector(text_input)

            st.write("\n--- Detection Result ---")
            st.write("Verdict:", verdict)  # Still displaying the verdict

            # Display details but exclude 'label'
            st.write("Details:")
            for key, value in results.items():
                if key != "label":  # Ensure we don't display the label
                    st.write(f"  {key}: {value}")

            # Offer to humanize the text if AI-generated
            if verdict == "The Text is generated by AI." or verdict == "The Text is generated by Human.":
                if st.button("Humanize AI"):
                    with st.spinner('Humanizing text...'):
                        humanized = humanize_text(text_input)
                    st.write("\n--- Humanized Text ---")
                    st.write(humanized)

    elif option == "Audio":
        # Audio Input
        st.header("AI Voice Detection")
        uploaded_audio = st.file_uploader("Upload an audio file (.wav)", type=["wav"])

        if uploaded_audio:
            st.audio(uploaded_audio, format="audio/wav")
            try:
                audio_data, sr = librosa.load(uploaded_audio, sr=None)
                mfcc_features = extract_mfcc_features(audio_data, sr)

                model, scaler = load_model_and_scaler()

                if model and scaler:
                    mfcc_scaled = scaler.transform(mfcc_features.reshape(1, -1))
                    prediction = model.predict(mfcc_scaled)[0]

                    if prediction == 0:
                        st.success("Message : ✅ The uploaded audio is classified as **REAL**.")
                    else:
                        st.error("Message : ❌ The uploaded audio is classified as **FAKE**.")
            except Exception as e:
                st.error(f"Error processing audio file: {e}")

if __name__ == '__main__':
    main() 